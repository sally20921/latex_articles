

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Article by Seri $\bullet$ January 2021 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting


\title{Shallow Domain Adaptation} % Article title


\author{%
\textsc{Seri Lee}\thanks{A thank you or further information} \\[1ex] % Your name
\normalsize Seoul National University \\ % Your institution
\normalsize \href{mailto:sally20921@snu.ac.kr}{sally20921@snu.ac.kr} % Your email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%

%\begin{abstract}
%\noindent \blindtext % Dummy abstract text - replace \blindtext with your abstract text
%\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}
\cite{b1}. The performance of a supervised model is validated on test examples that are drawn from the same distribution as that of training examples.
However, when the learned model is evaluated on examples from a different domain, the performance suffers due to the distribution differences between the domain data.
Shai et al. stated that the performance of a classifier in a new domain (the target domain) depends on two factors: (1) the performance in its own domain (the source domain)
and (2) the discrepancy between the domains.
Given identical labels $Y_s = Y_t$ and the same feature space $X_s = X_t$, Domain Adaptation (DA) deals with minimizing the discrepancy between two domains.
The domain discrepancy can be in terms of the differences in the marginal distributions $P_s(x_s) \neq P_t(x_t)$ or the conditional distributions $P(y_s | x_s) \neq P(y_t | x_t)$.

Given very few labeled examples in the target domain (or no labeled exmaples at all), it is hard to estimate $P_t(y_t|x_t)$. 
Most DA approaches assume the domains to be correlated to some extent with $P(y_s | x_s) \approx P(y_t | x_t)$ and only focus on minimizing the marginal distribution differences.
This setting is known with many names in the adaptation literature, namely, Domain Shift, Dataset Shift, and Covariate Shift.
However, when the conditional distribution differences are so significant $P(y_s|x_s) \neq P(y_t | x_t)$, then this setting is more commonly known as Sample Selection Bias.

\section{A Condensed Review of DA Literature}
The Support Vector Machine (SVM) based frameworks are semi-supervised DA approaches that modify the SVM optimization framework to use labeled examples from both the domains simultaneously for adapting to the target domain data.
Yang et al. proposed Adaptive SVM where the SVM source model is regularized for the target examples. Domain Adaptive Support Vector Machine and Projective Model Transfer SVM are very similar adaptation approaches where the source SVM decision boundary is iteratively adapted to better classify the target samples.
Wu et al. proposed another SVM-based framework to determine relevant support vectors from the source domain for the target task.
Hoffman et al. introduced Maximum Margin Domain Transfer where a linear transformation that maps the target features to the source features is learned to account for the distribution mismatch.

The Subspace-based adaptation methods are Unsupervised DA methods that align the source and target features.
The Subpsace Alignment method has a closed-form solution that aligns the Eigen feature spaces obtained through PCA.
Similarly, manifold based DA methods minimize the distribution differences in a low-dimensional manifold. 
\bibliography{refs}
\bibliographystyle{plain}

%----------------------------------------------------------------------------------------

\end{document}